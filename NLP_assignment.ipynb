{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK4zt9uqxBoA",
        "outputId": "3f96771f-bab1-4b7c-b1d8-9c869651f25d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import time\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# --- SETUP: Download Required NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalizing and cleaning\n"
      ],
      "metadata": {
        "id": "yLAI-rovukeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. HELPER: Roman to Arabic Conversion\n",
        "# ==========================================\n",
        "def roman_to_int(roman):\n",
        "    \"\"\"Converts a Roman Numeral string to an integer string.\"\"\"\n",
        "    roman_map = {'I': 1, 'V': 5, 'X': 10, 'L': 50,\n",
        "                 'C': 100, 'D': 500, 'M': 1000}\n",
        "    total = 0\n",
        "    prev_value = 0\n",
        "\n",
        "    for char in reversed(roman.upper()):\n",
        "        value = roman_map.get(char, 0)\n",
        "        if value < prev_value:\n",
        "            total -= value\n",
        "        else:\n",
        "            total += value\n",
        "        prev_value = value\n",
        "    return str(total)\n",
        "\n",
        "def normalize_token(word):\n",
        "    \"\"\"\n",
        "    Checks if a word is a Roman Numeral and converts it.\n",
        "    Refuses to convert 'I' to avoid confusing the pronoun 'I' with the number '1'.\n",
        "    \"\"\"\n",
        "    # 1. Strict Regex for Roman Numerals\n",
        "    # Matches patterns like II, IV, XII, but not random words like 'MIX' or 'DIV' (unless purely uppercase)\n",
        "    roman_pattern = r\"^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\"\n",
        "\n",
        "    # 2. Safety Check: Must be Uppercase and match pattern\n",
        "    if word.isupper() and re.match(roman_pattern, word):\n",
        "        # EXCEPTION: Do not convert solitary \"I\" (Pronoun protection)\n",
        "        if word == \"I\":\n",
        "            return word\n",
        "        return roman_to_int(word)\n",
        "\n",
        "    return word"
      ],
      "metadata": {
        "id": "0ERq29tFuaEu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input file"
      ],
      "metadata": {
        "id": "Ky-AQpJfu7LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP & UTILITIES\n",
        "# ==========================================\n",
        "def download_input_file():\n",
        "    url = \"https://corpus.canterbury.ac.nz/descriptions/aliced29.txt\"\n",
        "    print(f\"Downloading {url}...\")\n",
        "    response = requests.get(url)\n",
        "    return response.content.decode('latin-1')\n",
        "\n",
        "def base_clean(text):\n",
        "    # 1. Remove Emoticons & Special Symbols (Keep words, space, dots)\n",
        "    text = re.sub(r'[^\\w\\s,.]', '', text)\n",
        "    # 2. Normalize Whitespace (\\s\\t\\n -> space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "I81GvpHiu6OD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK"
      ],
      "metadata": {
        "id": "popWcrKEvet4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FRAMEWORK 1: NLTK\n",
        "# ==========================================\n",
        "def run_nltk(raw_text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # A. Clean String\n",
        "    step1_text = base_clean(raw_text)\n",
        "\n",
        "    # B. Tokenize\n",
        "    sentences = sent_tokenize(step1_text)\n",
        "    tokens = word_tokenize(step1_text)\n",
        "\n",
        "    # C. Filter Stopwords & Convert Roman Numerals\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    clean_tokens = []\n",
        "\n",
        "    for w in tokens:\n",
        "        # Check if it's a useful word\n",
        "        if w.lower() not in stop_words and w.isalnum():\n",
        "            # ** NEW STEP: Convert Roman -> Arabic **\n",
        "            final_word = normalize_token(w)\n",
        "            clean_tokens.append(final_word)\n",
        "\n",
        "    # D. Outputs\n",
        "    cleaned_text_str = \" \".join(clean_tokens)\n",
        "    freq_dist = nltk.FreqDist(clean_tokens)\n",
        "    top_10 = freq_dist.most_common(10)\n",
        "\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time, cleaned_text_str, sentences, clean_tokens, top_10"
      ],
      "metadata": {
        "id": "l59RYb2Evcxu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextBlob"
      ],
      "metadata": {
        "id": "2dDTQWAgvn42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# FRAMEWORK 2: TextBlob\n",
        "# ==========================================\n",
        "def run_textblob(raw_text):\n",
        "    start_time = time.time()\n",
        "\n",
        "    step1_text = base_clean(raw_text)\n",
        "    blob = TextBlob(step1_text)\n",
        "\n",
        "    sentences = blob.sentences\n",
        "    tokens = blob.words\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    clean_tokens = []\n",
        "\n",
        "    for w in tokens:\n",
        "        if w.lower() not in stop_words and w.isalnum():\n",
        "            # ** NEW STEP: Convert Roman -> Arabic **\n",
        "            final_word = normalize_token(w)\n",
        "            clean_tokens.append(final_word)\n",
        "\n",
        "    cleaned_text_str = \" \".join(clean_tokens)\n",
        "    word_counts = Counter(clean_tokens)\n",
        "    top_10 = word_counts.most_common(10)\n",
        "\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time, cleaned_text_str, sentences, clean_tokens, top_10"
      ],
      "metadata": {
        "id": "7p-2vPZ5vmrf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "f-jkypbXv2GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FRAMEWORK 3: spaCy\n",
        "# ==========================================\n",
        "def run_spacy(raw_text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    nlp.max_length = 2000000\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    step1_text = base_clean(raw_text)\n",
        "    doc = nlp(step1_text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    clean_tokens = []\n",
        "    for token in doc:\n",
        "        if not token.is_stop and token.is_alpha:\n",
        "            # ** NEW STEP: Convert Roman -> Arabic **\n",
        "            # We use token.text to get the string\n",
        "            final_word = normalize_token(token.text)\n",
        "            clean_tokens.append(final_word)\n",
        "\n",
        "    cleaned_text_str = \" \".join(clean_tokens)\n",
        "    word_counts = Counter(clean_tokens)\n",
        "    top_10 = word_counts.most_common(10)\n",
        "\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time, cleaned_text_str, sentences, clean_tokens, top_10"
      ],
      "metadata": {
        "id": "2CRnXenov1F9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output"
      ],
      "metadata": {
        "id": "pd8WHmAlwK6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# MAIN EXECUTION\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Get Data\n",
        "    raw_text = download_input_file()\n",
        "    print(f\"File downloaded. Length: {len(raw_text)} chars.\")\n",
        "\n",
        "    # 2. Run All Frameworks\n",
        "    print(\"Running NLTK...\")\n",
        "    nltk_time, nltk_clean, nltk_sents, nltk_words, nltk_top10 = run_nltk(raw_text)\n",
        "\n",
        "    print(\"Running TextBlob...\")\n",
        "    blob_time, blob_clean, blob_sents, blob_words, blob_top10 = run_textblob(raw_text)\n",
        "\n",
        "    print(\"Running spaCy...\")\n",
        "    spacy_time, spacy_clean, spacy_sents, spacy_words, spacy_top10 = run_spacy(raw_text)\n",
        "\n",
        "    # 3. Generate Output Files (Using NLTK results)\n",
        "\n",
        "    # Output A: cleaned.txt\n",
        "    with open(\"cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(nltk_clean)\n",
        "\n",
        "    # Output B: words.txt\n",
        "    with open(\"words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"--- SENTENCES (Sample) ---\\n\")\n",
        "        for s in nltk_sents[:5]:\n",
        "            f.write(str(s) + \"\\n\")\n",
        "        f.write(\"\\n--- TOKENS (Sample) ---\\n\")\n",
        "        f.write(str(nltk_words[:20]))\n",
        "\n",
        "    # Output C: top10words.txt\n",
        "    with open(\"top10words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"Rank | Word | Frequency\\n\")\n",
        "        f.write(\"-\" * 30 + \"\\n\")\n",
        "        for i, (word, count) in enumerate(nltk_top10, 1):\n",
        "            f.write(f\"{i} | {word} | {count}\\n\")\n",
        "\n",
        "    # Output D: time_compares.txt\n",
        "    with open(\"time_compares.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"Framework Performance Comparison\\n\")\n",
        "        f.write(f\"================================\\n\")\n",
        "        f.write(f\"NLTK Time     : {nltk_time:.4f} seconds\\n\")\n",
        "        f.write(f\"TextBlob Time : {blob_time:.4f} seconds\\n\")\n",
        "        f.write(f\"spaCy Time    : {spacy_time:.4f} seconds\\n\")\n",
        "\n",
        "    print(\"\\nDONE! Roman numerals (like II, VII) have been converted to Arabic (2, 7).\")\n",
        "    print(\"Files generated: cleaned.txt, words.txt, top10words.txt, time_compares.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZKStWW7wJrS",
        "outputId": "f49d1a72-9c0f-4bd6-cdb6-bb464d62d498"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://corpus.canterbury.ac.nz/descriptions/aliced29.txt...\n",
            "File downloaded. Length: 323 chars.\n",
            "Running NLTK...\n",
            "Running TextBlob...\n",
            "Running spaCy...\n",
            "\n",
            "DONE! Roman numerals (like II, VII) have been converted to Arabic (2, 7).\n",
            "Files generated: cleaned.txt, words.txt, top10words.txt, time_compares.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}